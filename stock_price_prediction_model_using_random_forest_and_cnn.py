# -*- coding: utf-8 -*-
"""Stock price prediction model using Random Forest and CNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_Uigy_NRhHeutkxptAkVVsVGWgt_N5Fx
"""

!pip install scipy

!pip install scikit-learn

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split

# Step 1: Load the dataset
file_path = "/content/stock_price_prediction.csv"
df = pd.read_csv(file_path)  # Replace 'file_path' with the path to your CSV file

stockdata = df.drop(columns=['ticker', 'Unnamed: 0'], axis=1)

stockdata.head()

import pandas as pd

# Assuming you have your stock dataset loaded into a pandas DataFrame called 'stock_data'
# For demonstration purposes, I'm assuming your DataFrame has columns "Outstanding_share" and "turnover"

# Drop one null value from "Outstanding_share" column
stockdata.dropna(subset=['outstanding_share'], inplace=True, how='any')

# Drop one null value from "turnover" column
stockdata.dropna(subset=['turnover'], inplace=True, how='any')

stockdata.isnull().sum()

stockdata.shape

# Extract features and target (stock price) columns
X= stockdata.drop(columns=['date', 'close'])  # Assuming 'Date' column is present
y = df['close']

X_test.isnull().sum()

X.shape

import numpy as np

# Assuming you have your data loaded in variables X and y

# Remove the last row from y
y = y[:-1]

# Now X and y have the same number of samples (179204)
print("X shape:", X.shape)  # Output: (179204, 6)
print("y shape:", y.shape)  # Output: (179204,)

y.shape

from sklearn.model_selection import train_test_split

# Assuming you have your data loaded into variables X and y

# Split the data into training and testing sets with the same length for y_test and y_train
test_size = 0.2  # You can adjust this value as needed
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)

# Now both y_test and y_train have the same number of samples
print("y_train length:", len(y_train))
print("y_test length:", len(y_test))

y_test.shape

# Initialize the Random Forest model
from sklearn.ensemble import RandomForestRegressor
model = RandomForestRegressor(n_estimators=100, random_state=42)

# Train the model
model.fit(X_train, y_train)



from sklearn.metrics import r2_score

# Drop the 'Unnamed: 0' feature from X_test, if it exists
X_test = X_test.drop(columns=['Unnamed: 0'], errors='ignore')

# Now, you can proceed with making predictions
y_pred = model.predict(X_test)

# Calculate the R-squared score
print("R-squared score for the model: ", r2_score(y_test, y_pred))

import pickle
from sklearn.ensemble import RandomForestRegressor

# Convert the model to a pickled binary format
pickled_model = pickle.dumps(model)

# Now the model is in a pickled binary format, and you can save and download it
# Specify the path where you want to save the pickled model
model_save_path = "/content/Random Forest model"  # Replace with your desired file path and name

# Save the pickled model to a file
with open(model_save_path, "wb") as file:
    file.write(pickled_model)

print("Random Forest model saved as a pickled file.")

import joblib
from sklearn.ensemble import RandomForestRegressor

# Specify the path where you want to save the model
model_save_path = "/content/Random Forest model"  # Replace with your desired file path and name

# Save the model
joblib.dump(model, model_save_path)

print("Random Forest model saved successfully.")

!pip install keras

!pip install tensorflow

import numpy as np
import pandas as pd
import tensorflow as tf
from sklearn.model_selection import train_test_split

# Load and preprocess your stock dataset (assuming you have it loaded into a DataFrame)
# Preprocess your data here...

# Split the data into features (X) and target variable (y)
X= stockdata.drop(columns=['date', 'close'])  # Assuming 'Date' column is present
y = df['close'].shift(-1)[:-1]


# Convert X and y to numpy arrays
X = X.values
y = y.values

# Reshape X to be 3D (samples, time steps, features)
X = X.reshape(X.shape[0], X.shape[1], 1)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Build the CNN model
cnn_model = tf.keras.Sequential([
    tf.keras.layers.Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=(X.shape[1], 1)),
    tf.keras.layers.MaxPooling1D(pool_size=2),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(units=64, activation='relu'),
    tf.keras.layers.Dense(units=1)  # Output layer, units=1 for regression
])

# Compile the model
cnn_model.compile(optimizer='adam', loss='mean_squared_error')

# Train the model
cnn_model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2)

# Evaluate the model
test_loss = cnn_model.evaluate(X_test, y_test)
print("Test Loss:", test_loss)

# Make predictions
predictions = cnn_model.predict(X_test)

# Make predictions
predictions = cnn_model.predict(X_test)

from sklearn.metrics import accuracy_score
# Evaluate the model and get accuracy
test_accuracy = cnn_model.evaluate(X_test, y_test)
print("Test Accuracy:", test_accuracy)